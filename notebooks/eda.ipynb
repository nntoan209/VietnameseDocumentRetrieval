{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_dir=\"data/train_generic/msmarco_squadv2_splitted\",\n",
    "                    data_files=[file for file in os.listdir(\"data/train_generic/msmarco_squadv2_splitted\") if file.endswith(\".jsonl\")],\n",
    "                    split=\"train\", num_proc=32, cache_dir=\"./.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_dir=\"data/train_generic/structured_data_doc_splitted\",\n",
    "                    data_files=[file for file in os.listdir(\"data/train_generic/structured_data_doc_splitted\") if file.endswith(\".jsonl\")],\n",
    "                    split=\"train\", num_proc=32, cache_dir=\"./.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_dir=\"data/train_generic/newssapo_splitted\",\n",
    "                    data_files=[file for file in os.listdir(\"data/train_generic/newssapo_splitted\") if file.endswith(\".jsonl\")],\n",
    "                    split=\"train\", num_proc=32, cache_dir=\"./.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = [\"data/train_generic/msmarco_squadv2_splitted\"]\n",
    "\n",
    "for data_folder in data_folders:\n",
    "    for file in sorted(os.listdir(data_folder))[2:]:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            data_file = os.path.join(data_folder, file)\n",
    "            print(data_file)\n",
    "            dataset = load_dataset(\"json\", data_files=[data_file],\n",
    "                                   split=\"train\", num_proc=32, cache_dir=\"./.cache\").select_columns([\"query\", \"pos\"])\n",
    "            dataset.to_json(data_file, orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "from datasets import load_dataset\n",
    "\n",
    "train_group_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_group_size == 2:\n",
    "    context_feat = datasets.Features({\n",
    "        'query': datasets.Value('string'),\n",
    "        'pos': datasets.Sequence(datasets.Value('string'))\n",
    "    })\n",
    "else:\n",
    "    context_feat = datasets.Features({\n",
    "        'query': datasets.Value('string'),\n",
    "        'pos': datasets.Sequence(datasets.Value('string')),\n",
    "        'neg': datasets.Sequence(datasets.Value('string'))\n",
    "    })\n",
    "context_feat_kd = datasets.Features({\n",
    "    'query': datasets.Value('string'),\n",
    "    'pos': datasets.Sequence(datasets.Value('string')),\n",
    "    'neg': datasets.Sequence(datasets.Value('string')),\n",
    "    'pos_scores': datasets.Sequence(datasets.Value('float')),\n",
    "    'neg_scores': datasets.Sequence(datasets.Value('float')),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\"data/train_generic/newssapo_splitted\"]\n",
    "\n",
    "for data_dir in train_data:\n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"{data_dir} is a file, not a directionary\")\n",
    "\n",
    "    # Add `parallel_` in `data_dir` to indicate that this dataset is parallel corpus\n",
    "    flag = 'parallel_' in data_dir\n",
    "    for file in os.listdir(data_dir):\n",
    "        if not (file.endswith('.json') or file.endswith('.jsonl')):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        try:\n",
    "            temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', features=context_feat)\n",
    "        except:\n",
    "            temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', features=context_feat_kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from rank_bm25 import BM25Plus\n",
    "from BGE_M3.process_data.utils import bm25_tokenizer\n",
    "\n",
    "def convert_score_to_rank(scores):\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    ranked_indices = np.argsort(sorted_indices) + 1\n",
    "    return ranked_indices\n",
    "\n",
    "dev_queries = json.load(open(\"data/eval/data/dev_queries.json\", encoding='utf-8'))\n",
    "formatted_dev_queries = {\"id\": list(dev_queries.keys()), \"text\": list(dev_queries.values())}\n",
    "dev_queries_dataset = Dataset.from_dict(formatted_dev_queries)\n",
    "\n",
    "corpus = json.load(open(\"data/eval/data/filtered_corpus.json\", encoding='utf-8'))\n",
    "bm25 = BM25Plus(corpus=list(corpus.values()),\n",
    "                tokenizer=bm25_tokenizer,\n",
    "                k1=0.4,\n",
    "                b=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranks(sample):\n",
    "    scores = bm25.get_scores(bm25_tokenizer(sample['text']))\n",
    "    ranks = convert_score_to_rank(scores)\n",
    "    return {\"ranks\": ranks}\n",
    "\n",
    "dev_queries_ranks = dev_queries_dataset.map(calculate_ranks, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from BGE_M3.process_data.utils import bm25_tokenizer\n",
    "from rank_bm25 import BM25Plus\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Loading BM25 model ...\")\n",
    "with open(\"saved_models/bm25result_1.25_0.9\", 'rb') as bm25result_file:\n",
    "    bm25 = pickle.load(bm25result_file)\n",
    "    \n",
    "print(\"Loading dev queries and corpus ...\")\n",
    "corpus = json.load(open(\"data/eval/law/filtered_corpus.json\", encoding='utf-8'))\n",
    "dev_queries = json.load(open(\"data/eval/law/dev_queries.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_score_to_rank(scores):\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    ranked_indices = np.argsort(sorted_indices) + 1\n",
    "    return ranked_indices\n",
    "\n",
    "def calculate_ranks(sample):\n",
    "    scores = bm25.get_scores(bm25_tokenizer(sample))\n",
    "    ranks = convert_score_to_rank(scores)\n",
    "    return ranks\n",
    "print(\"Calculate bm25 ranks ...\")\n",
    "    \n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    sparse_ranks = np.array(list(executor.map(calculate_ranks, dev_queries.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "queries_embeddings = torch.rand(9992, 1024)\n",
    "corpus_embeddings = torch.rand(240000, 1024)\n",
    "\n",
    "# queries_embeddings = np.random.default_rng().standard_normal(size=(9992, 1024), dtype='float32')\n",
    "# corpus_embeddings = np.random.default_rng().standard_normal(size=(240000, 1024), dtype='float32')\n",
    "\n",
    "results_dense_search = semantic_search(queries_embeddings, corpus_embeddings,\n",
    "                                       top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BGE_M3.src.utils import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(\n",
    "    model_name_or_path=\"/home/admin_mcn/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/babcf60cae0a1f438d7ade582983d4ba462303c2\",\n",
    "    pooling_method=\"cls\",\n",
    "    use_fp16=False,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "sentences_1 = [\"What is panda?\", \"What is BGE M3?\", \"Defination of BM25\", \"What is a codebase (code base)?\", \"What is a large language model (LLM)?\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\",\n",
    "               \"BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\",\n",
    "               \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "               \"A codebase is the complete body of source code for a given software program, component or application system.\",\n",
    "               \"Large language models (LLMs) are machine learning models that can comprehend and generate human language text.\"]\n",
    "\n",
    "query_embeddings = model.encode(sentences_1)\n",
    "passage_embeddings = model.encode(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "scores1 = torch.rand(9992, 224000).cpu().numpy()\n",
    "scores2 = torch.rand(9992, 224000).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BGE_M3.eval.utils import ranks_from_scores_parallel\n",
    "\n",
    "ranks1 = ranks_from_scores_parallel(scores1, num_workers=32)\n",
    "ranks2 = ranks_from_scores_parallel(scores2, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from BGE_M3.src.utils import rrf_from_scores\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def parallel_calculate_rrf(scores1, scores2, k=60, num_workers=4):\n",
    "    Nq, Np = scores1.shape\n",
    "    \n",
    "    # Split the queries into chunks\n",
    "    print(\"splitting chunks\")\n",
    "    chunk_size = 100\n",
    "    chunks = [(scores1[i:i + chunk_size], scores2[i:i + chunk_size]) \n",
    "              for i in range(0, Nq, chunk_size)]\n",
    "    \n",
    "    # Process each chunk in parallel using multiprocessing Pool\n",
    "    print(\"processing chunks\")\n",
    "    with Parallel(n_jobs=num_workers, verbose=10) as parallel:\n",
    "        results = parallel(delayed(rrf_from_scores)(scores1_chunk, scores2_chunk, k) \n",
    "                           for scores1_chunk, scores2_chunk in chunks)\n",
    "    \n",
    "    # Concatenate the results\n",
    "    print(\"concatenating results\")\n",
    "    rrf_scores = np.zeros((Nq, Np))\n",
    "    for i, chunk in enumerate(results):\n",
    "        rrf_scores[i*chunk_size:(i+1)*chunk_size] = chunk\n",
    "    \n",
    "    return rrf_scores\n",
    "\n",
    "# Example usage\n",
    "scores1_test = scores1\n",
    "scores2_test = scores2\n",
    "\n",
    "k = 10\n",
    "num_workers = 8  # Adjust the number of workers according to your system's capabilities\n",
    "rrf_scores = parallel_calculate_rrf(scores1_test, scores2_test, k, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BGE_M3.src.utils import rrf_from_scores_chunk, rrf_from_scores_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_chunk = rrf_from_scores_chunk(\n",
    "    scores1, scores2, k=10, bm25_weight=0.1, chunk_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rrf_from_scores_parallel(scores1, scores2, 10, 0.1,\n",
    "                                  tool=\"joblib\", num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.delete_file(\n",
    "    path_in_repo=\"law_translated-10-of-112.json\",\n",
    "    repo_id=\"nntoan209/law_translated\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=\"nntoan209/bgem3-vi-e3\",\n",
    "    repo_type=\"model\",\n",
    "    folder_path=\"saved_models/vi/bgem3_vi_20240618/checkpoint-25638\",\n",
    "    ignore_patterns=\"global_step25638*\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id=\"nntoan209/TrainData-CrossLingual\",\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=\"data/cross_lingual\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import datasets\n",
    "import math\n",
    "from pprint import pprint\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from transformers import DataCollatorWithPadding\n",
    "from easydict import EasyDict\n",
    "\n",
    "class LawsTrainDatasetCrossLingual(Dataset):\n",
    "    \"\"\"Dataset to yield a batch of data at one time. All samples in the same batch comes from the same task.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: EasyDict, batch_size: int, seed: int, process_index: int=0, num_processes: int=1):\n",
    "        \n",
    "        train_datasets = []\n",
    "        each_data_inxs = []\n",
    "        batch_size_inxs = []\n",
    "        pqloss_flag = []\n",
    "        cur_all_num = 0\n",
    "        \n",
    "        SMALL_THRESHOLD = args.small_threshold\n",
    "        DROP_THRESHOLD = args.drop_threshold\n",
    "        \n",
    "        if args.train_group_size == 1:\n",
    "            context_feat = datasets.Features({\n",
    "                'query': datasets.Value('string'),\n",
    "                'pos': datasets.Sequence(datasets.Value('string'))\n",
    "            })\n",
    "        else:\n",
    "            context_feat = datasets.Features({\n",
    "                'query': datasets.Value('string'),\n",
    "                'pos': datasets.Sequence(datasets.Value('string')),\n",
    "                'neg': datasets.Sequence(datasets.Value('string'))\n",
    "            })\n",
    "        context_feat_kd = datasets.Features({\n",
    "            'query': datasets.Value('string'),\n",
    "            'pos': datasets.Sequence(datasets.Value('string')),\n",
    "            'neg': datasets.Sequence(datasets.Value('string')),\n",
    "            'pos_scores': datasets.Sequence(datasets.Value('float')),\n",
    "            'neg_scores': datasets.Sequence(datasets.Value('float')),\n",
    "        })\n",
    "        assert isinstance(args.train_data, list) and len(args.train_data) >= 1\n",
    "        \n",
    "        self.print_batch_size(batch_size=batch_size, train_group_size=args.train_group_size)\n",
    "        \n",
    "        for data_dir in args.train_data:\n",
    "            if not os.path.isdir(data_dir):\n",
    "                raise FileNotFoundError(f\"{data_dir} is a file, not a directionary\")\n",
    "            \n",
    "            small_datasets = []\n",
    "            small_batch_size = math.inf\n",
    "            \n",
    "            # Add `parallel_` in `data_dir` to indicate that this dataset is parallel corpus\n",
    "            flag = 'parallel_' in data_dir\n",
    "            for file in os.listdir(data_dir):\n",
    "                if not (file.endswith('.json') or file.endswith('.jsonl')):\n",
    "                    continue\n",
    "                \n",
    "                file_path = os.path.join(data_dir, file)\n",
    "                print(f'loading data from {file_path} ...')\n",
    "                try:\n",
    "                    temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', cache_dir=args.cache_path, features=context_feat)\n",
    "                except:\n",
    "                    temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', cache_dir=args.cache_path, features=context_feat_kd)\n",
    "                    if not args.knowledge_distillation:\n",
    "                        temp_dataset = temp_dataset.remove_columns(['pos_scores', 'neg_scores'])\n",
    "                \n",
    "                if len(temp_dataset) == 0:\n",
    "                    continue\n",
    "                elif len(temp_dataset) < SMALL_THRESHOLD:\n",
    "                    small_datasets.append(temp_dataset)\n",
    "                    small_batch_size = min(small_batch_size, self.get_file_batch_size(file, batch_size, train_group_size=args.train_group_size))\n",
    "                else:\n",
    "                    if args.max_example_num_per_dataset is not None and len(temp_dataset) > args.max_example_num_per_dataset:\n",
    "                        temp_dataset = temp_dataset.select(\n",
    "                            random.sample(list(range(len(temp_dataset))), args.max_example_num_per_dataset))\n",
    "                    train_datasets.append(temp_dataset)\n",
    "                    each_data_inxs.append(np.arange(len(temp_dataset)) + cur_all_num)\n",
    "                    cur_all_num += len(temp_dataset)\n",
    "                    batch_size_inxs.append(self.get_file_batch_size(file, batch_size, train_group_size=args.train_group_size))\n",
    "                    pqloss_flag.append(flag)\n",
    "            \n",
    "            if len(small_datasets) > 0:\n",
    "                small_dataset = datasets.concatenate_datasets(small_datasets)\n",
    "                if len(small_dataset) >= DROP_THRESHOLD:\n",
    "                    train_datasets.append(small_dataset)\n",
    "                    each_data_inxs.append(np.arange(len(small_dataset)) + cur_all_num)\n",
    "                    cur_all_num += len(small_dataset)\n",
    "                    batch_size_inxs.append(small_batch_size)\n",
    "                    pqloss_flag.append(flag)\n",
    "        \n",
    "        self.dataset = datasets.concatenate_datasets(train_datasets)\n",
    "        self.each_data_inxs = each_data_inxs\n",
    "        self.datasets_inxs = np.arange(len(each_data_inxs))\n",
    "        self.batch_size_inxs = batch_size_inxs\n",
    "        self.pqloss_flag = pqloss_flag\n",
    "        \n",
    "        self.process_index = process_index\n",
    "        self.num_processes = num_processes\n",
    "        self.args = args\n",
    "        self.shuffle_ratio = args.shuffle_ratio\n",
    "        \n",
    "        self.deterministic_generator = np.random.default_rng(seed)\n",
    "        self.step = 0\n",
    "        self.refresh_epoch()\n",
    "    \n",
    "    def print_batch_size(self, batch_size: int, train_group_size: int):\n",
    "        length_list = ['0-512', '512-1024', '1024-2048', '2048-inf']\n",
    "        batch_size_dict = {\n",
    "            k: self.get_file_batch_size(f\"len-{k}.jsonl\", batch_size, train_group_size) for k in length_list\n",
    "        }\n",
    "        batch_size_list = [\n",
    "            f'{length}: {batch_size_dict[length]}' for length in length_list\n",
    "        ]\n",
    "        print(\"=========================\")\n",
    "        print(\"Batch Size Dict:\")\n",
    "        pprint(batch_size_list)\n",
    "        print(\"=========================\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_file_batch_size(file: str, batch_size: int, train_group_size: int):\n",
    "        if train_group_size == 3:\n",
    "            # 24GB\n",
    "            if 'len-0-512.jsonl' in file:\n",
    "                return 60\n",
    "            elif 'len-512-1024.jsonl' in file:\n",
    "                return 44\n",
    "            elif 'len-1024-2048.jsonl' in file:\n",
    "                return 24\n",
    "            elif 'len-2048-inf.jsonl' in file:\n",
    "                return 24\n",
    "            else:\n",
    "                return batch_size\n",
    "        elif train_group_size == 2:\n",
    "            # 24GB\n",
    "            if 'len-0-512.jsonl' in file:\n",
    "                return 88\n",
    "            elif 'len-512-1024.jsonl' in file:\n",
    "                return 58\n",
    "            elif 'len-1024-2048.jsonl' in file:\n",
    "                return 24\n",
    "            elif 'len-2048-inf.jsonl' in file:\n",
    "                return 16\n",
    "            else:\n",
    "                return batch_size\n",
    "        else:\n",
    "            return batch_size\n",
    "        \n",
    "                \n",
    "    def refresh_epoch(self):\n",
    "        print(f'---------------------------*Rank {self.process_index}: refresh data---------------------------')\n",
    "        self.deterministic_generator.shuffle(self.datasets_inxs)\n",
    "        # Dynamically adjust batch size\n",
    "        batch_datas = []\n",
    "        for dataset_inx in self.datasets_inxs:\n",
    "            self.deterministic_generator.shuffle(self.each_data_inxs[dataset_inx])\n",
    "            cur_batch_size = self.batch_size_inxs[dataset_inx]*self.num_processes\n",
    "            flag = self.pqloss_flag[dataset_inx]\n",
    "            for start_index in range(0, len(self.each_data_inxs[dataset_inx]), cur_batch_size):\n",
    "                # judge the last batch's length\n",
    "                if len(self.each_data_inxs[dataset_inx]) - start_index < 2 * self.num_processes:\n",
    "                    break\n",
    "                batch_datas.append((self.each_data_inxs[dataset_inx][start_index:start_index+cur_batch_size], flag))\n",
    "        self.deterministic_generator.shuffle(batch_datas)\n",
    "        self.batch_datas = batch_datas\n",
    "        self.step = 0\n",
    "\n",
    "    def __getitem__(self, _):  \n",
    "        batch_indices, pqloss_flag = self.batch_datas[self.step]\n",
    "        cur_batch_size = int(len(batch_indices) / self.num_processes)\n",
    "        batch_indices = batch_indices[self.process_index * cur_batch_size: (self.process_index + 1) * cur_batch_size]\n",
    "        batch_data = self.dataset[batch_indices]\n",
    "        self.step += 1\n",
    "        queries, passages, teacher_scores = self.create_batch_data(batch_raw_data=batch_data)\n",
    "        # print('rank, step, flag, query, passage:', dist.get_rank(), self.step, pqloss_flag, queries, passages)\n",
    "        return queries, passages, teacher_scores, pqloss_flag\n",
    "\n",
    "    def shuffle_text(self, text):\n",
    "        if self.shuffle_ratio > 0 and len(text) > 100 and random.random() < self.shuffle_ratio:\n",
    "            split_text = []\n",
    "            chunk_size = len(text)//3 + 1\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                split_text.append(text[i:i+chunk_size])\n",
    "            random.shuffle(split_text)\n",
    "            return \" \".join(split_text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def create_batch_data(self, batch_raw_data):\n",
    "        queries, passages = [], []\n",
    "        teacher_scores = []\n",
    "        for i in range(len(batch_raw_data['query'])):            \n",
    "            queries.append(batch_raw_data['query'][i])\n",
    "            \n",
    "            pos_inx = random.choice(list(range(len(batch_raw_data['pos'][i]))))\n",
    "            passages.append(self.shuffle_text(batch_raw_data['pos'][i][pos_inx]))\n",
    "            if 'pos_scores' in batch_raw_data and batch_raw_data['pos_scores'][i] is not None:\n",
    "                teacher_scores.append(batch_raw_data['pos_scores'][i][pos_inx])\n",
    "            \n",
    "            if self.args.train_group_size > 1:\n",
    "                neg_inx_set = list(range(len(batch_raw_data['neg'][i])))\n",
    "                if len(batch_raw_data['neg'][i]) < self.args.train_group_size - 1:\n",
    "                    num = math.ceil((self.args.train_group_size - 1) / len(batch_raw_data['neg'][i]))\n",
    "                    neg_inxs = random.sample(neg_inx_set * num, self.args.train_group_size - 1)\n",
    "                else:\n",
    "                    neg_inxs = random.sample(neg_inx_set, self.args.train_group_size - 1)            \n",
    "                \n",
    "                if 'neg_scores' in batch_raw_data and batch_raw_data['neg_scores'][i] is not None:\n",
    "                    neg_scores = [(x, batch_raw_data['neg_scores'][i][x]) for x in neg_inxs]\n",
    "                    neg_scores = sorted(neg_scores, key=lambda x:x[1], reverse=True)\n",
    "                    neg_inxs = [x[0] for x in neg_scores]\n",
    "                    teacher_scores.extend([x[1] for x in neg_scores])\n",
    "                    \n",
    "                negs = [batch_raw_data['neg'][i][x] for x in neg_inxs]\n",
    "                passages.extend(negs)\n",
    "            \n",
    "            if len(teacher_scores) > 0 and len(passages) > 0:\n",
    "                assert len(teacher_scores) == len(passages)\n",
    "\n",
    "        if self.args.query_instruction_for_retrieval is not None:\n",
    "            queries = [self.args.query_instruction_for_retrieval+q for q in queries]\n",
    "        if self.args.passage_instruction_for_retrieval is not None:\n",
    "            passages = [self.args.passage_instruction_for_retrieval+p for p in passages]\n",
    "        \n",
    "        if len(teacher_scores) == 0:\n",
    "            teacher_scores = None\n",
    "        return queries, passages, teacher_scores\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batch_datas) * self.num_processes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbedCollatorCrossLingual(DataCollatorWithPadding):\n",
    "    \"\"\"\n",
    "    Wrapper that does conversion from List[Tuple[encode_qry, encode_psg]] to List[qry], List[psg]\n",
    "    and pass batch separately to the actual collator.\n",
    "    Abstract out data detail for the model.\n",
    "    \"\"\"\n",
    "    query_max_len: int = 32\n",
    "    passage_max_len: int = 128\n",
    "    merged_queries_vi: dict = None\n",
    "    merged_corpus_vi: dict = None\n",
    "    merged_queries_en: dict = None\n",
    "    merged_corpus_en: dict = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        query = [f[0] for f in features]\n",
    "        passage = [f[1] for f in features]\n",
    "        \n",
    "        teacher_scores = None\n",
    "        if len(features[0]) > 2:\n",
    "            teacher_scores = [f[2] for f in features]\n",
    "            if teacher_scores[0] is None:\n",
    "                teacher_scores = None\n",
    "            else:\n",
    "                teacher_scores = torch.FloatTensor(teacher_scores)\n",
    "        \n",
    "        flag = None\n",
    "        if len(features[0]) == 4:\n",
    "            flag = [f[3] for f in features][0]\n",
    "            \n",
    "        if isinstance(query[0], list):\n",
    "            query = sum(query, [])\n",
    "        if isinstance(passage[0], list):\n",
    "            passage = sum(passage, [])\n",
    "            \n",
    "        # random between vietnamese and english\n",
    "        # query = [self.merged_queries_vi[q] if random.random() <= 0.5 else self.merged_queries_en[q] for q in query]\n",
    "        # passage = [self.merged_corpus_vi[p] if random.random() <= 0.5 else self.merged_corpus_en[p] for p in passage]\n",
    "        query = [self.merged_queries_vi[q] for q in query]\n",
    "        passage = [self.merged_corpus_vi[p] for p in passage]\n",
    "\n",
    "        q_collated = self.tokenizer(\n",
    "            query,\n",
    "            # padding='max_length',     # used for adjusting the batch size in `get_file_batch_size()`\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.query_max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        d_collated = self.tokenizer(\n",
    "            passage,\n",
    "            # padding='max_length',     # used for adjusting the batch size in `get_file_batch_size()`\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.passage_max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if teacher_scores is not None:\n",
    "            teacher_scores = teacher_scores.reshape((len(q_collated['input_ids']), -1))\n",
    "        return {\"query\": query, \"passage\": passage}\n",
    "        return {\"query\": q_collated, \"passage\": d_collated, \"teacher_scores\": teacher_scores, \"bi_directions\": flag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "data_args = EasyDict()\n",
    "data_args.small_threshold = 0\n",
    "data_args.drop_threshold = 0\n",
    "data_args.train_group_size = 2\n",
    "data_args.train_data = [\"data/cross_lingual/merged_train_data_splitted\"]\n",
    "data_args.knowledge_distillation = False\n",
    "data_args.cache_path = \"./.cache\"\n",
    "data_args.max_example_num_per_dataset = None\n",
    "data_args.shuffle_ratio = 0\n",
    "data_args.query_instruction_for_retrieval = None\n",
    "data_args.passage_instruction_for_retrieval = None\n",
    "data_args.merged_queries_vi = \"data/cross_lingual/merged_queries_vi.json\"\n",
    "data_args.merged_queries_en = \"data/cross_lingual/merged_queries_en.json\"\n",
    "data_args.merged_corpus_vi = \"data/cross_lingual/merged_corpus_vi.json\"\n",
    "data_args.merged_corpus_en = \"data/cross_lingual/merged_corpus_en.json\"\n",
    "\n",
    "\n",
    "merged_queries_vi = json.load(open(data_args.merged_queries_vi, encoding='utf-8'))\n",
    "merged_corpus_vi = json.load(open(data_args.merged_corpus_vi, encoding='utf-8'))\n",
    "merged_queries_en = json.load(open(data_args.merged_queries_en, encoding='utf-8'))\n",
    "merged_corpus_en = json.load(open(data_args.merged_corpus_en, encoding='utf-8'))\n",
    "\n",
    "train_dataset = LawsTrainDatasetCrossLingual(args=data_args, \n",
    "                                            batch_size=1, \n",
    "                                            seed=1234, \n",
    "                                            num_processes=1,\n",
    "                                            process_index=0)\n",
    "\n",
    "data_collator = EmbedCollatorCrossLingual(\n",
    "        tokenizer,\n",
    "        merged_queries_vi=merged_queries_vi,\n",
    "        merged_corpus_vi=merged_corpus_vi,\n",
    "        merged_queries_en=merged_queries_en,\n",
    "        merged_corpus_en=merged_corpus_en,\n",
    "        query_max_len=128,\n",
    "        passage_max_len=2048\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(train_dataset, collate_fn=data_collator, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(data_loader):\n",
    "    a = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a['query']), len(a['passage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpl_corpus = json.load(open(\"data/eval/zalo_qa/filtered_corpus.json\", encoding='utf-8'))\n",
    "tvpl_corpus_dict = {\"id\": list(tvpl_corpus.keys()), \"text\": list(tvpl_corpus.values())}\n",
    "tvpl_corpus_dataset = Dataset.from_dict(tvpl_corpus_dict)\n",
    "tvpl_corpus_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpl_corpus_dataset = tvpl_corpus_dataset.map(lambda x: {\"length\": len(tokenizer(x['text'])['input_ids'])}, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tvpl_corpus_dataset.filter(lambda x: 1024 <= x['length'] < 2048, num_proc=32)) / len(tvpl_corpus_dataset) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_count = {}\n",
    "\n",
    "with open(\"../data/cross_lingual/merged_train_data_ids.jsonl\") as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line)\n",
    "        dataset_name = data['query'].split('_')[0]\n",
    "        \n",
    "        train_sample_count[dataset_name] = train_sample_count.get(dataset_name, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvpl 165347\n",
      "zalolegal 2556\n",
      "msmarco 457361\n",
      "squadv2 60942\n"
     ]
    }
   ],
   "source": [
    "for k, v in train_sample_count.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "law 9992\n",
      "zalo_legal 640\n",
      "zalo_qa 4399\n"
     ]
    }
   ],
   "source": [
    "eval_dataset_names = [\"law\", \"zalo_legal\", \"zalo_qa\"]\n",
    "for name in eval_dataset_names:\n",
    "    dev_rel_docs = json.load(open(f\"../data/eval/{name}/dev_rel_docs.json\", encoding='utf-8'))\n",
    "    print(name, len(dev_rel_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "law 224008\n",
      "zalo_legal 61060\n",
      "zalo_qa 15957\n"
     ]
    }
   ],
   "source": [
    "for name in [\"law\", \"zalo_legal\", \"zalo_qa\"]:\n",
    "    filtered_corpus = json.load(open(f\"../data/eval/{name}/filtered_corpus.json\", encoding='utf-8'))\n",
    "    print(name, len(filtered_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rel_docs = {\n",
    "    \"tvpl\": [],\n",
    "    \"zalolegal\": [],\n",
    "    \"msmarco\": [],\n",
    "    \"squadv2\": []\n",
    "}\n",
    "\n",
    "with open(\"../data/cross_lingual/merged_train_data_ids.jsonl\") as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line)\n",
    "        dataset_name = data['query'].split('_')[0]\n",
    "        \n",
    "        num_rel_docs[dataset_name].append(len(data['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 60942})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(num_rel_docs['squadv2'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "515 + 112 + 16 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3460, 2: 682, 3: 175, 4: 52, 5: 19, 6: 7, 7: 4})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "dev_rel_docs = json.load(open(\"../data/eval/zalo_qa/dev_rel_docs.json\", encoding='utf-8'))\n",
    "for k, v in dev_rel_docs.items():\n",
    "    l.append(len(v))\n",
    "Counter(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'squadv2_4778',\n",
       " 'pos': ['squadv2_768'],\n",
       " 'neg': ['squadv2_702',\n",
       "  'squadv2_767',\n",
       "  'squadv2_696',\n",
       "  'squadv2_766',\n",
       "  'squadv2_712']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_ids = load_dataset(\"json\", data_files=\"../data/cross_lingual/merged_train_data_ids.jsonl\", split=\"train\")\n",
    "train_ids[630042]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m queries_vi \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cross_lingual/merged_queries_vi.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m queries_en \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cross_lingual/merged_queries_en.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m corpus_vi \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/cross_lingual/merged_corpus_vi.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m corpus_en \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cross_lingual/merged_corpus_en.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/toannn/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/toannn/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "queries_vi = json.load(open(\"../data/cross_lingual/merged_queries_vi.json\", encoding='utf-8'))\n",
    "queries_en = json.load(open(\"../data/cross_lingual/merged_queries_en.json\", encoding='utf-8'))\n",
    "corpus_vi = json.load(open(\"../data/cross_lingual/merged_corpus_vi.json\", encoding='utf-8'))\n",
    "corpus_en = json.load(open(\"../data/cross_lingual/merged_corpus_en.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Kanye West đã bán được tổng cộng bao nhiêu bài hát kỹ thuật số ở Mỹ?',\n",
       " 'Sáu album phòng thu solo đầu tiên của West, tất cả đều đạt đĩa bạch kim, đã nhận được nhiều giải thưởng và sự hoan nghênh của giới phê bình. Tất cả các album của anh đều thành công về mặt thương mại, trong đó Yeezus, album solo thứ sáu của anh, trở thành album quán quân thứ năm liên tiếp tại Mỹ khi phát hành. West đã có sáu bài hát vượt quá 3 triệu lượt bán kỹ thuật số tính đến tháng 12 năm 2012, với \"Gold Digger\" bán được 3.086.000, \"Stronger\" bán được 4.402.000, \"Heartless\" bán được 3.742.000, \"E.T.\" bán được hơn 4.000.000, \"Love Lockdown\" bán được hơn 3.000.000 và \"Niggas in Paris\" bán được hơn 3.000.000, giúp anh đứng thứ ba về tổng doanh số bán kỹ thuật số trong thập kỷ qua. Anh đã bán được hơn 30 triệu bài hát kỹ thuật số tại Hoa Kỳ, khiến anh trở thành một trong những nghệ sĩ kỹ thuật số bán chạy nhất mọi thời đại.',\n",
       " 'West đã dành phần lớn thời gian cuối những năm 1990 để sản xuất đĩa hát cho một số nghệ sĩ và nhóm nhạc nổi tiếng. Bài hát thứ ba trong album phòng thu thứ hai Chyna Doll của Foxy Brown do West sản xuất. Nỗ lực thứ hai của cô sau đó đã trở thành album hip-hop đầu tiên của một nữ rapper đứng đầu bảng xếp hạng Billboard 200 của Hoa Kỳ trong tuần đầu tiên phát hành. West đã sản xuất ba bài hát trong album đầu tiên và duy nhất The Movement của Harlem World cùng với Jermaine Dupri và bộ đôi sản xuất Trackmasters. Các bài hát của anh có sự góp mặt của rapper Nas, Drag-On và ca sĩ R&B Carl Thomas. Ca khúc thứ chín của World Party, album Goodie Mob cuối cùng có sự góp mặt của bốn thành viên sáng lập nhóm nhạc rap trước khi họ tan rã, do West đồng sản xuất với người quản lý Deric \"D-Dot\" Angelettie. Vào cuối thiên niên kỷ, West đã sản xuất sáu bài hát cho Tell \\'Em Why U Madd, một album được phát hành bởi D-Dot với bí danh The Madd Rapper; một nhân vật hư cấu mà anh ấy đã tạo ra cho một tiểu phẩm trong album phòng thu thứ hai và cũng là cuối cùng của The Notorious B.I.G. Life After Death. Các bài hát của West có sự xuất hiện của các rapper như Ma$e, Raekwon và Eminem.')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid = 'squadv2_4778'\n",
    "pid = 'squadv2_768'\n",
    "npid = 'squadv2_702'\n",
    "queries_vi[qid], corpus_vi[pid], corpus_vi[npid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toannn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
