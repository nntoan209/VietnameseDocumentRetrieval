{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = {}\n",
    "\n",
    "zalo_corpus = load_dataset(\"parquet\", data_dir=\"../data/tvpl_new/dedup\",\n",
    "                            data_files=\"zalo_legal_corpus.parquet\",\n",
    "                            split=\"train\",\n",
    "                            num_proc=16)\n",
    "\n",
    "for item in tqdm(zalo_corpus):\n",
    "    original_corpus[item['oid']] = item['title'] + \" \" + item['text']\n",
    "\n",
    "sft_corpus = load_dataset(\"parquet\", data_dir=\"../data/tvpl_new/dedup\",\n",
    "                    data_files=[\"sft_train.parquet\", \"sft_test.parquet\"],\n",
    "                    split=\"train\",\n",
    "                    num_proc=16)\n",
    "\n",
    "for item in tqdm(sft_corpus):\n",
    "    original_corpus[item['oid']] = item['text']\n",
    "\n",
    "tvpl_corpus = load_dataset(\"parquet\", data_dir=\"../data/tvpl_new/dedup\",\n",
    "                            data_files=[\"tvpl_dataset.parquet\"],\n",
    "                            split=\"train\",\n",
    "                            num_proc=16)\n",
    "\n",
    "for law in tqdm(tvpl_corpus):\n",
    "    for item in law['child_data']:\n",
    "        original_corpus[item['oid']] = item['text']\n",
    "\n",
    "with open(\"../data/final/corpus/original_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(original_corpus, fOut, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus = load_dataset(\"parquet\", data_dir=\"../data/tvpl_new/dedup\",\n",
    "                               data_files=\"filtered_corpus.parquet\",\n",
    "                               split=\"train\",\n",
    "                               num_proc=16)\n",
    "\n",
    "filtered_corpus_indexed = {}\n",
    "for item in tqdm(filtered_corpus):\n",
    "    filtered_corpus_indexed[str(item['__cluster__'])] = original_corpus[str(item['oid'])]\n",
    "\n",
    "with open(\"../data/final/corpus/filtered_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(filtered_corpus_indexed, fOut, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = load_dataset(\"parquet\",\n",
    "                          data_dir=\"../data/tvpl_new/dedup/newtraintestdivide\",\n",
    "                          data_files=\"train.parquet\",\n",
    "                          split=\"train\",\n",
    "                          num_proc=16)\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for item in tqdm(train_split):\n",
    "    train_sample = {}\n",
    "    train_sample['query'] = item['question']\n",
    "    train_sample['pos'] = [original_corpus[str(item['oid'])]]\n",
    "\n",
    "    train_data.append(train_sample)\n",
    "\n",
    "with open(\"../data/final/train/train_data.jsonl\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    for data in train_data:\n",
    "        fOut.write(json.dumps(data, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = load_dataset(\"parquet\",\n",
    "                          data_dir=\"../data/tvpl_new/dedup/newtraintestdivide\",\n",
    "                          data_files=\"test.parquet\",\n",
    "                          split=\"train\",\n",
    "                          num_proc=16)\n",
    "\n",
    "dev_queries = {}\n",
    "dev_rel_docs = {}\n",
    "\n",
    "index = 0\n",
    "for item in tqdm(test_split):\n",
    "    dev_queries[f\"query_{index}\"] = item['question']\n",
    "    dev_rel_docs[f\"query_{index}\"] = str(item['__cluster__'])\n",
    "\n",
    "    index += 1\n",
    "\n",
    "with open(\"../data/final/test/dev_queries.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries, fOut, indent=2)\n",
    "\n",
    "with open(\"../data/final/test/dev_rel_docs.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_rel_docs, fOut, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data_doc = load_dataset(\"parquet\", data_files=\"../data/tvpl_new/structured_data_doc.parquet\",\n",
    "                                   split=\"train\", num_proc=32)\n",
    "structured_data_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a temporary dataset, each item is a chunk in sample['data']\n",
    "temp_dataset = structured_data_doc.map(lambda x: {\"chunk_text\": [chunk_data['full_text'] for chunk_data in x['data'][0]]},\n",
    "                                       batched=True, batch_size=1, remove_columns=structured_data_doc.column_names, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\".\", \";\", \",\", \" \", \"\"],\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=0,\n",
    "    length_function=lambda x: len(tokenizer(x)[\"input_ids\"]),\n",
    "    is_separator_regex=False,\n",
    "    keep_separator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def process_dataset(sample):\n",
    "    retry = 0\n",
    "    texts = text_splitter.split_text(sample['chunk_text'])\n",
    "    \n",
    "    # sample query and positive\n",
    "    if len(texts) == 1:\n",
    "        return {\"query\": None, \"pos\": None}\n",
    "    query, pos = random.sample(texts, 2)\n",
    "    while (len(tokenizer(query)['input_ids']) < 64) or (len(tokenizer(pos)['input_ids']) < 64):\n",
    "        if retry > 500:\n",
    "            retry = 0\n",
    "            return {\"query\": None, \"pos\": None}\n",
    "        query, pos = random.sample(texts, 2)\n",
    "        retry += 1\n",
    "    \n",
    "    # remove leading punctuation\n",
    "    query = query.lstrip(punctuation + \" \")\n",
    "    pos = pos.lstrip(punctuation + \" \")\n",
    "\n",
    "    return {\"query\": query, \"pos\": [pos]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = temp_dataset.map(process_dataset,\n",
    "                                 remove_columns=\"chunk_text\",\n",
    "                                 num_proc=32)\n",
    "\n",
    "def filter_func(sample):\n",
    "    if (sample['query'] is None) or (sample['pos'] is None):\n",
    "        return False\n",
    "    if len(tokenizer(sample['query'])['input_ids']) > 8192 or len(tokenizer(sample['pos'][0])['input_ids']) > 8192:\n",
    "        return False\n",
    "    return True\n",
    "final_dataset = final_dataset.filter(filter_func, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_json(\"../data/final/train/generic/structured_data_doc.jsonl\", orient=\"records\", lines=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New SFT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "train_data = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/tvpl_sft_resplit/train.parquet\",\n",
    "                          split=\"train\", num_proc=16)\n",
    "\n",
    "test_data = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/tvpl_sft_resplit/test.parquet\",\n",
    "                         split=\"train\", num_proc=16)\n",
    "\n",
    "filtered_corpus = json.load(open(\"../data/eval/law/filtered_corpus.json\", encoding='utf-8'))\n",
    "# filtered_corpus = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/merged_corpus/filtered_corpus.parquet\", split=\"train\")\n",
    "reindexed_corpus = json.load(open(\"../data/eval/law/reindexed_corpus.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus['41938']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_corpus['61437']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus_indexed = {}\n",
    "for item in tqdm(filtered_corpus):\n",
    "    filtered_corpus_indexed[str(item['__cluster__'])] = item['text'].replace(u'\\xa0' or '\\u200b', '')\n",
    "\n",
    "with open(\"../data/eval/law/filtered_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(filtered_corpus_indexed, fOut, indent=2)\n",
    "\n",
    "reindexed_corpus_indexed = {}\n",
    "for item in tqdm(reindexed_corpus):\n",
    "    reindexed_corpus_indexed[str(item['oid'])] = item['text'].replace(u'\\xa0' or '\\u200b', '')\n",
    "    \n",
    "with open(\"../data/eval/law/reindexed_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(reindexed_corpus_indexed, fOut, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_queries = {}\n",
    "dev_rel_docs = {}\n",
    "\n",
    "index = 0\n",
    "for item in tqdm(test_data):\n",
    "    if item['question'] not in dev_queries:\n",
    "        dev_queries[item['question']] = f\"query_{index}\"\n",
    "        dev_rel_docs[f\"query_{index}\"] = set(str(cluster) for cluster in item['__context_cluster__'])\n",
    "        index += 1\n",
    "    else:\n",
    "        for p in item['__context_cluster__']:\n",
    "            dev_rel_docs[dev_queries[item['question']]].add(str(p))\n",
    "\n",
    "dev_queries = {v: k for k,v in dev_queries.items()}\n",
    "dev_rel_docs = {k: list(v) for k, v in dev_rel_docs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/law/dev_queries.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries, fOut, indent=2)\n",
    "\n",
    "with open(\"../data/eval/law/dev_rel_docs.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_rel_docs, fOut, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "flag = 0\n",
    "for data in tqdm(train_data):\n",
    "    for c in data[\"__context_cluster__\"]:\n",
    "        if str(c) not in filtered_corpus:\n",
    "            a.append(data['context'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean the train data\n",
    "def map_func(sample):\n",
    "    assert len(sample['__context_cluster__']) == len(sample['contextoid'])\n",
    "    new_contextoid = []\n",
    "    new_context_cluster = []\n",
    "    for contextoid, context_cluster in zip(sample['contextoid'], sample['__context_cluster__']):\n",
    "        if str(context_cluster) in filtered_corpus:\n",
    "            new_contextoid.append(contextoid)\n",
    "            new_context_cluster.append(context_cluster)\n",
    "    sample['contextoid'] = new_contextoid\n",
    "    sample['__context_cluster__'] = new_context_cluster\n",
    "    return sample\n",
    "\n",
    "train_data_clean = train_data.map(map_func, num_proc=16).filter(lambda x: len(x['__context_cluster__']) >= 1, num_proc=16)\n",
    "train_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {}\n",
    "\n",
    "for item in tqdm(train_data_clean):\n",
    "    if item['question'] not in train_data_dict:\n",
    "        train_data_dict[item['question']] = [set(item['contextoid']), set(item['__context_cluster__'])]\n",
    "    else:\n",
    "        for contextoid in item['contextoid']:\n",
    "            train_data_dict[item['question']][0].add(contextoid)\n",
    "        for context_cluster in item['__context_cluster__']:\n",
    "            train_data_dict[item['question']][1].add(context_cluster)\n",
    "\n",
    "train_data_json = []\n",
    "for k, v in train_data_dict.items():\n",
    "    train_data_json.append({\"query\": k,\n",
    "                            \"pos\": [reindexed_corpus[str(context_id)] for context_id in v[0]],\n",
    "                            \"__context_cluster__\": list(v[1])})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_json[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sft/train_data.jsonl\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    for data in train_data_json:\n",
    "        fOut.write(json.dumps(data, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=\"../data/sft/train_data_minedHN_new.jsonl\", split=\"train\", cache_dir=\"../.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "invert_filtered_corpus = {v: k for k, v in filtered_corpus.items()}\n",
    "\n",
    "for i, item in tqdm(enumerate(ds)):\n",
    "    for neg in item['neg']:\n",
    "        if neg in item['pos']:\n",
    "            print(item)\n",
    "            break\n",
    "        if invert_filtered_corpus[neg] in train_data_json[i]['__context_cluster__']:\n",
    "            print(item)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Answer, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_data = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/tvpl_sft_resplit/test.parquet\",\n",
    "                         split=\"train\", num_proc=16)\n",
    "\n",
    "filtered_corpus = json.load(open(\"../data/eval/law/filtered_corpus.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_queries = {}\n",
    "dev_queries_answer = {}\n",
    "\n",
    "index = 0\n",
    "for item in tqdm(test_data):\n",
    "    if item['question'] not in dev_queries:\n",
    "        dev_queries[item['question']] = f\"query_{index}\"\n",
    "        dev_queries_answer[f\"query_{index}\"] = set([item['long_answer']])\n",
    "        index += 1\n",
    "    else:\n",
    "        for p in item['__context_cluster__']:\n",
    "            dev_queries_answer[dev_queries[item['question']]].add(item['long_answer'])\n",
    "\n",
    "dev_queries = {v: k for k,v in dev_queries.items()}\n",
    "dev_queries_answer = {k: list(v) for k, v in dev_queries_answer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/law/dev_answers.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries_answer, fOut, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/eval/law/dev_answers.json\",\n",
    "    repo_id=\"nntoan209/LawEval\",\n",
    "    path_in_repo=\"dev_answers.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"../saved_models/bgem3_sft_20240601/checkpoint-29610\",\n",
    "    repo_id=\"nntoan209/bgem3-sft-msmarco-squadv2-1gpu-e3\",\n",
    "    repo_type=\"model\",\n",
    "    ignore_patterns=\"global_step29610/*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"../data/sft/splitted\",\n",
    "    repo_id=\"nntoan209/LawFinetuneV2\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "for file in os.listdir(\"../data/sft/tvpl_splitted\"):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        print(file)\n",
    "        ds = load_dataset(\"json\", data_files=f\"../data/sft/tvpl_splitted/{file}\", split=\"train\")\n",
    "        filtered_ds = ds.filter(lambda x: len(x['pos']) >= 1, num_proc=32)\n",
    "        \n",
    "        # Save the filter_ds\n",
    "        if len(filtered_ds) < len(ds):\n",
    "            print(f\"Filtered {len(ds) - len(filtered_ds)} samples\")\n",
    "            filtered_ds.to_json(f\"../data/sft/tvpl_splitted/{file}\", orient=\"records\", lines=True, num_proc=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalo legal 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_question_answer = json.load(open(\"../data/original/zalo_legal_2021/train_question_answer.json\", encoding=\"utf-8\"))['items'][-640:]\n",
    "filtered_corpus = json.load(open(\"../data/eval/law/filtered_corpus.json\", encoding='utf-8'))\n",
    "zalo_legal_corpus = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/data_remapped/zalo_legal_corpus\", split=\"train\")\n",
    "reindexed_corpus = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/merged_corpus/reindexed_corpus.parquet\", split=\"train\").filter(lambda x: x['dataset'] == \"zalo_legal_corpus\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalo_legal_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus_to_zalo_corpus = {str(i): [] for i in filtered_corpus.keys()}\n",
    "for item in tqdm(zalo_legal_corpus):\n",
    "    zalo_id = item['law_id'] + \"_\" + item['article_id']\n",
    "    if str(item['__cluster__']) in filtered_corpus_to_zalo_corpus:\n",
    "        filtered_corpus_to_zalo_corpus[str(item['__cluster__'])].append(zalo_id)\n",
    "for k, v in filtered_corpus_to_zalo_corpus.items():\n",
    "    filtered_corpus_to_zalo_corpus[k] = list(set(v))\n",
    "    if len(filtered_corpus_to_zalo_corpus[k]) == 0:\n",
    "        filtered_corpus_to_zalo_corpus[k] = [\"NOT FOUND\"]\n",
    "len(filtered_corpus_to_zalo_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/filtered_corpus_to_zalo_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(filtered_corpus_to_zalo_corpus, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "dev_queries = {}\n",
    "dev_rel_docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalo_legal_corpus_indexed = {}\n",
    "for item in tqdm(zalo_legal_corpus):\n",
    "    article_id = item['article_id']\n",
    "    law_id = item['law_id']\n",
    "    \n",
    "    zalo_legal_corpus_indexed[str(item['oid'])] = [f\"{law_id}_{article_id}\", item['text'].replace(u'\\xa0' or '\\u200b', '')]\n",
    "    \n",
    "for item in tqdm(reindexed_corpus):\n",
    "    a = zalo_legal_corpus_indexed[str(item['oid'])]\n",
    "    corpus[a[0]] = a[1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/filtered_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(corpus, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_answer in tqdm(train_question_answer):\n",
    "    rel_docs = []\n",
    "    for article in question_answer['relevant_articles']:\n",
    "        law_id = article['law_id']\n",
    "        article_id = article['article_id']\n",
    "        rel_docs.append(f\"{law_id}_{article_id}\")\n",
    "    \n",
    "    dev_queries[question_answer['question_id']] = question_answer['question']\n",
    "    dev_rel_docs[question_answer['question_id']] = rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/dev_queries.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries, fOut, indent=2, ensure_ascii=False)\n",
    "with open(\"../data/eval/zalo_legal/dev_rel_docs.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_rel_docs, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalo QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/train_generic/zaloqa/train.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "dev_queries = {}\n",
    "dev_rel_docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_index = 0\n",
    "\n",
    "for item in data:\n",
    "    if item['text'] not in corpus:\n",
    "        corpus[item['text']] = f\"doc_{corpus_index}\"\n",
    "        corpus_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 0\n",
    "for item in data:\n",
    "    if item['question'] not in dev_queries and item['label']:\n",
    "        dev_queries[item['question']] = f\"query_{query_index}\"\n",
    "        dev_rel_docs[f\"query_{query_index}\"] = [corpus[item['text']]]\n",
    "        query_index += 1\n",
    "    elif item['question'] in dev_queries and item['label']:\n",
    "        dev_rel_docs[dev_queries[item['question']]].append(corpus[item['text']])\n",
    "        \n",
    "dev_queries = {v: k for k, v in dev_queries.items()}\n",
    "dev_rel_docs = {k: list(set(v)) for k, v in dev_rel_docs.items()}\n",
    "corpus = {v: k for k, v in corpus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_rel_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [item['text'] for item in data]\n",
    "len(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(dev_queries[f\"query_{i}\"])\n",
    "list(corpus[i] for i in dev_rel_docs[f\"query_{i}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_qa/dev_queries.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries, fOut, ensure_ascii=False, indent=2)\n",
    "with open(\"../data/eval/zalo_qa/dev_rel_docs.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_rel_docs, fOut, indent=2)\n",
    "with open(\"../data/eval/zalo_qa/filtered_corpus.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(corpus, fOut, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"json\", data_files=\"../data/sft/train_data.jsonl\",\n",
    "                          split=\"train\", num_proc=16)\n",
    "\n",
    "dev_queries = json.load(open(\"../data/eval/law/dev_queries.json\", encoding=\"utf-8\"))\n",
    "zalo_qa_queries = json.load(open(\"../data/eval/zalo_qa/dev_queries.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_to_translate = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for item in train_data:\n",
    "    queries_to_translate[f\"train_{index}\"] = item[\"query\"]\n",
    "    index += 1\n",
    "    \n",
    "for k, v in dev_queries.items():\n",
    "    queries_to_translate[f\"dev_{k}\"] = v\n",
    "\n",
    "for k, v in zalo_qa_queries.items():\n",
    "    queries_to_translate[f\"zaloqa_{k}\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/queries_to_translate.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(queries_to_translate, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/eval/queries_to_translate.json\",\n",
    "    repo_id=\"nntoan209/law_queries_translated\",\n",
    "    path_in_repo=\"queries_to_translate.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "queries_en = json.load(open(\"../data/law_translated/queries/queries_en.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_queries_en = {}\n",
    "train_queries_en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in queries_en.items():\n",
    "    if k.startswith(\"train\"):\n",
    "        query_id = k.split(\"_\")[1]\n",
    "        train_queries_en[query_id] = v\n",
    "    elif k.startswith(\"dev\"):\n",
    "        query_id = \"_\".join(k.split(\"_\")[1:])\n",
    "        dev_queries_en[query_id] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the train_queries_en and dev_queries_en by keys:\n",
    "train_queries_en = {k: train_queries_en[k] for k in sorted(train_queries_en, key=lambda x: int(x))}\n",
    "dev_queries_en = {k: dev_queries_en[k] for k in sorted(dev_queries_en, key=lambda x: int(x.split(\"_\")[-1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/eval/law/train_queries_en.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(train_queries_en, fOut, indent=2, ensure_ascii=False)\n",
    "with open(\"../data/eval/law/dev_queries_en.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(dev_queries_en, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_queries_vi = json.load(open(\"../data/law_translated/queries/queries_vi.json\", encoding='utf-8'))\n",
    "all_queries_en = json.load(open(\"../data/law_translated/queries/queries_en.json\"))\n",
    "all_queries_vi_invert = {v: k for k, v in all_queries_vi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpl_queries = {}\n",
    "tvpl_queries_en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165347it [00:00, 205591.12it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/sft/train_data_minedHN_v3.jsonl\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        item = json.loads(line)\n",
    "\n",
    "        query_idx = all_queries_vi_invert[item['query']]\n",
    "        tvpl_queries[query_idx] = item['query']\n",
    "        tvpl_queries_en[query_idx] = all_queries_en[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/law/train_queries.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(tvpl_queries, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "with open(\"../data/eval/law/train_queries_en.json\", \"w\") as f:\n",
    "    json.dump(tvpl_queries_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate positive passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "train_data = load_dataset(\"parquet\", data_files=\"../data/original/tvpl/dedup/tvpl_sft_resplit/train.parquet\",\n",
    "                          split=\"train\", num_proc=16)\n",
    "\n",
    "filtered_corpus = json.load(open(\"../data/eval/law/filtered_corpus.json\", encoding='utf-8'))\n",
    "filtered_corpus_invert = {v: k for k, v in filtered_corpus.items()}\n",
    "reindexed_corpus = json.load(open(\"../data/eval/law/reindexed_corpus.json\", encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean the train data\n",
    "def map_func(sample):\n",
    "    assert len(sample['__context_cluster__']) == len(sample['contextoid'])\n",
    "    new_contextoid = []\n",
    "    new_context_cluster = []\n",
    "    for contextoid, context_cluster in zip(sample['contextoid'], sample['__context_cluster__']):\n",
    "        # Remove the __context_cluster__ that is not in the filtered corpus\n",
    "        if str(context_cluster) in filtered_corpus:\n",
    "            new_contextoid.append(contextoid)\n",
    "            new_context_cluster.append(context_cluster)\n",
    "    sample['contextoid'] = new_contextoid\n",
    "    sample['__context_cluster__'] = new_context_cluster\n",
    "    return sample\n",
    "\n",
    "# Remove the samples that have no positive passages\n",
    "train_data_clean = train_data.map(map_func, num_proc=16).filter(lambda x: len(x['__context_cluster__']) >= 1, num_proc=16)\n",
    "train_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {}\n",
    "\n",
    "for item in tqdm(train_data_clean):\n",
    "    if item['question'] not in train_data_dict:\n",
    "        train_data_dict[item['question']] = [item['contextoid'], item['__context_cluster__']]\n",
    "    else:\n",
    "        for contextoid in item['contextoid']:\n",
    "            train_data_dict[item['question']][0].append(contextoid)\n",
    "        for context_cluster in item['__context_cluster__']:\n",
    "            train_data_dict[item['question']][1].append(context_cluster)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tqdm(train_data_dict.items()):\n",
    "    context_oid = list(v[0])\n",
    "    context_cluster = list(v[1])\n",
    "    \n",
    "    new_context_oid = []\n",
    "    for oid, cluster in zip(context_oid, context_cluster):\n",
    "        if reindexed_corpus[str(oid)] != filtered_corpus[str(cluster)]:\n",
    "            new_context_oid.append(f\"oid_{oid}\")\n",
    "        else:\n",
    "            new_context_oid.append(f\"cluster_{cluster}\")\n",
    "            \n",
    "    train_data_dict[k] = (list(set(new_context_oid)), list(set(context_cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict[list(train_data_dict.keys())[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_json = []\n",
    "for k, v in train_data_dict.items():\n",
    "    train_data_json.append({\"query\": k,\n",
    "                            \"pos\": v[0],\n",
    "                            \"__context_cluster__\": v[1]})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sft/train_data.jsonl\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    for data in train_data_json:\n",
    "        fOut.write(json.dumps(data, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_passages_to_translate = {}\n",
    "for v in train_data_dict.values():\n",
    "    for pid in v[0]:\n",
    "        if pid.startswith(\"oid_\"):\n",
    "            additional_passages_to_translate[pid] = reindexed_corpus[pid.split(\"_\")[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_passages_to_translate['oid_61426']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/law_translated/additional_passages_to_translate.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(additional_passages_to_translate, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/law_translated/additional_passages_to_translate.json\",\n",
    "    repo_id=\"nntoan209/additional_law_translated\",\n",
    "    path_in_repo=\"additional_passages_to_translate.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "ds = load_dataset(\"json\", data_dir=\"../data/law_translated/corpus\",\n",
    "                  data_files=[file for file in os.listdir(\"../data/law_translated/corpus\") if file.startswith(\"additional\")],\n",
    "                  split=\"train\", num_proc=16)\n",
    "\n",
    "ds_to_sort = ds.map(lambda x: {\"doc_id\": int(x['id'].split(\"_\")[-1])}, num_proc=16).sort(\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_corpus_en = {}\n",
    "for item in ds_to_sort:\n",
    "    reindexed_corpus_en[str(item['doc_id'])] = item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/law/reindexed_corpus_en.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(reindexed_corpus_en, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalo QA corpus translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "zaloqa_corpus = json.load(open(\"../data/eval/zalo_qa/filtered_corpus.json\", encoding=\"utf-8\"))\n",
    "zaloqa_corpus['doc_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zaloqa_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/eval/zalo_qa/filtered_corpus.json\",\n",
    "    repo_id=\"nntoan209/zaloqa_corpus_translated\",\n",
    "    path_in_repo=\"filtered_corpus.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "ds = load_dataset(\"json\", data_dir=\"../data/law_translated/corpus\",\n",
    "                  data_files=[file for file in os.listdir(\"../data/law_translated/corpus\") if file.startswith(\"zaloqa_corpus\")],\n",
    "                  split=\"train\", num_proc=16)\n",
    "\n",
    "ds_to_sort = ds.map(lambda x: {\"doc_id\": int(x['id'].split(\"_\")[-1])}, num_proc=16).sort(\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_queries_en = {}\n",
    "filtered_corpus_en = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds_to_sort:\n",
    "    filtered_corpus_en[item['id']] = item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/eval/zalo_qa/filtered_corpus_en.json\", \"w\") as f:\n",
    "    json.dump(filtered_corpus_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/law_translated/queries/queries_en.json\") as f:\n",
    "    queries_en = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in queries_en.items():\n",
    "    if k.startswith(\"zaloqa_\"):\n",
    "        query_id = \"_\".join(k.split(\"_\")[1:])\n",
    "        dev_queries_en[query_id] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/eval/zalo_qa/dev_queries_en.json\", \"w\") as f:\n",
    "    json.dump(dev_queries_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate Zalo Legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "zalolegal_train_data = json.load(open(\"../data/original/zalo_legal_2021/train_question_answer.json\", encoding=\"utf-8\"))['items']\n",
    "zalolegal_corpus = json.load(open(\"../data/eval/zalo_legal/filtered_corpus.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/eval/zalo_legal/filtered_corpus.json\",\n",
    "    path_in_repo=\"filtered_corpus.json\",\n",
    "    repo_id=\"nntoan209/zalolegal_corpus_translated\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalolegal_queries = {}\n",
    "for item in zalolegal_train_data:\n",
    "    zalolegal_queries[item['question_id']] = item['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/law_translated/zalolegal_queries_vi.json\", \"w\", encoding=\"utf-8\") as fOut:\n",
    "    json.dump(zalolegal_queries, fOut, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"../data/law_translated/zalolegal_queries_vi.json\",\n",
    "    path_in_repo=\"zalolegal_queries_vi.json\",\n",
    "    repo_id=\"nntoan209/zalolegal_queries_translated\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_queries_en = json.load(open(\"../data/law_translated/queries/zalolegal_queries_translated-1-of-1.json\"))\n",
    "all_queries_en = {item['id']: item['text'] for item in all_queries_en}\n",
    "dev_queries_vi = json.load(open(\"../data/eval/zalo_legal/dev_queries.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_queries_en = {}\n",
    "for k in dev_queries_vi.keys():\n",
    "    dev_queries_en[k] = all_queries_en[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/dev_queries_en.json\", \"w\") as f:\n",
    "    json.dump(dev_queries_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "filtered_corpus = json.load(open(\"../data/eval/zalo_legal/filtered_corpus.json\", encoding=\"utf-8\"))\n",
    "ds = load_dataset(\"json\", data_dir=\"../data/law_translated/corpus\",\n",
    "                  data_files=[file for file in os.listdir(\"../data/law_translated/corpus\") if file.startswith(\"zalolegal_corpus\")],\n",
    "                  split=\"train\", num_proc=8)\n",
    "ds = {item['id']: item['text'] for item in ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus_en = {}\n",
    "for k in filtered_corpus.keys():\n",
    "    filtered_corpus_en[k] = ds[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/filtered_corpus_en.json\", \"w\") as f:\n",
    "    json.dump(filtered_corpus_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    repo_id=\"nntoan209/ZaloLegal-CrossLingual\",\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=\"../data/eval/zalo_legal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "zalolegal_train = json.load(open(\"../data/original/zalo_legal_2021/train_question_answer.json\", encoding=\"utf-8\"))['items'][:-640]\n",
    "all_queries_en = json.load(open(\"../data/law_translated/queries/zalolegal_queries_translated-1-of-1.json\"))\n",
    "all_queries_en = {item['id']: item['text'] for item in all_queries_en}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = {}\n",
    "train_queries_en = {}\n",
    "zalolegal_train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in zalolegal_train:\n",
    "    train_queries[item['question_id']] = item['question']\n",
    "    train_queries_en[item['question_id']] = all_queries_en[item['question_id']]\n",
    "    \n",
    "    rel_docs = []\n",
    "    for doc in item['relevant_articles']:\n",
    "        rel_docs.append(f\"{doc['law_id']}_{doc['article_id']}\")\n",
    "    zalolegal_train_data.append({\"query\": item['question'],\n",
    "                                 \"pos\": rel_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sft/zalolegal_train_data.jsonl\", \"w\") as f:\n",
    "    for item in zalolegal_train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/eval/zalo_legal/train_queries.json\", \"w\") as f:\n",
    "    json.dump(train_queries, f, indent=2, ensure_ascii=False)\n",
    "with open(\"../data/eval/zalo_legal/train_queries_en.json\", \"w\") as f:\n",
    "    json.dump(train_queries_en, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TVPL and Zalo Legal train data to all ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tvpl_train_queries = json.load(open(\"../data/eval/law/train_queries.json\", encoding='utf-8'))\n",
    "tvpl_train_queries_invert = {v: k for k, v in tvpl_train_queries.items()}\n",
    "tvpl_train_data = []\n",
    "with open(\"../data/sft/train_data_minedHN_v3.jsonl\", encoding='utf-8') as fIn:\n",
    "    for line in fIn:\n",
    "        item = json.loads(line)\n",
    "        query_id = tvpl_train_queries_invert[item['query']]\n",
    "        new_pos = []\n",
    "        for p in item['pos']:\n",
    "            if p.startswith(\"oid_\"):\n",
    "                new_pos.append(p)\n",
    "            elif p.startswith(\"cluster_\"):\n",
    "                new_p = p.split(\"_\")[-1]\n",
    "                new_pos.append(new_p)\n",
    "                \n",
    "        tvpl_train_data.append({\"query\": query_id,\n",
    "                                \"pos\": new_pos,\n",
    "                                \"neg\": item['neg']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sft/train_data_minedHN_v3_ids.jsonl\", \"w\") as f:\n",
    "    for item in tvpl_train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalolegal_train_queries = json.load(open(\"../data/eval/zalo_legal/train_queries.json\", encoding='utf-8'))\n",
    "zalolegal_train_queries_invert = {v: k for k, v in zalolegal_train_queries.items()}\n",
    "zalolegal_train_data = []\n",
    "with open(\"../data/sft/zalolegal_train_data_minedHN.jsonl\", encoding='utf-8') as fIn:\n",
    "    for line in fIn:\n",
    "        item = json.loads(line)\n",
    "        query_id = zalolegal_train_queries_invert[item['query']]\n",
    "        zalolegal_train_data.append({\"query\": query_id,\n",
    "                                     \"pos\": item['pos'],\n",
    "                                     \"neg\": item['neg']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sft/zalolegal_train_data_minedHN_ids.jsonl\", \"w\") as f:\n",
    "    for item in zalolegal_train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toannn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
