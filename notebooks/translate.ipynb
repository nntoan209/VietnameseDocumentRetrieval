{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DATASET = 1\n",
    "PART_DATASET = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NUM_CPU = os.cpu_count()\n",
    "HF_TOKEN = \"hf_\"\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    MOUNT_PATH = \"/content/drive\"\n",
    "else:\n",
    "    MOUNT_PATH = \".\"\n",
    "    \n",
    "EXTEND_PATH = \"../data/law_translated\"\n",
    "    \n",
    "CURRENT_FOLDER_PATH = os.path.join(MOUNT_PATH, EXTEND_PATH)\n",
    "os.makedirs(CURRENT_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "\n",
    "dataset = json.load(open(\"../data/eval/law/filtered_corpus.json\", encoding='utf-8'))\n",
    "dataset = {\"id\": list(dataset.keys()), \"text\": list(dataset.values())}\n",
    "dataset = datasets.Dataset.from_dict(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import typing as tp\n",
    "import unicodedata\n",
    "\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sentence_splitter import SentenceSplitter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenize_with_fillers(text, splitter, fix_double_space=True, ignore_errors=False):\n",
    "    \"\"\"Apply a sentence splitter and return the sentences and all separators before and after them\"\"\"\n",
    "    if fix_double_space:\n",
    "        text = re.sub(\" +\", \" \", text)\n",
    "        text = re.sub(r\"\\.+\", \".\", text)\n",
    "    sentences = splitter.split(text)\n",
    "    fillers = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        start_idx = text.find(sentence, i)\n",
    "        if ignore_errors and start_idx == -1:\n",
    "            # print(f\"sent not found after {i}: `{sentence}`\")\n",
    "            start_idx = i + 1\n",
    "        assert start_idx != -1, f\"sent not found after {i}: `{sentence}`\"\n",
    "        fillers.append(text[i:start_idx])\n",
    "        i = start_idx + len(sentence)\n",
    "    fillers.append(text[i:])\n",
    "    return sentences, fillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Mimic the text preprocessing made for the NLLB model.\n",
    "    This code is adapted from the Stopes repo of the NLLB team:\n",
    "    https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lang=\"en\"):\n",
    "        self.mpn = MosesPunctNormalizer(lang=lang)\n",
    "        self.mpn.substitutions = [\n",
    "            (re.compile(r), sub) for r, sub in self.mpn.substitutions\n",
    "        ]\n",
    "        self.replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        clean = self.mpn.normalize(text)\n",
    "        clean = self.replace_nonprint(clean)\n",
    "        # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "        clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "        return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "INITIAL_BATCH_SIZE = 8\n",
    "MAX_LENGTH = 328\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, model_url):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Running on GPU ...\")\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            print(\"WARNING. Running on CPU ...\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_url)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_url, src_lang=\"en_XX\")\n",
    "        self.splitter = SentenceSplitter(\"en\")\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "\n",
    "    def translate(self, text, max_length_generation=\"auto\",\n",
    "                  by_sentence=True, preprocess=True,\n",
    "                  translate_single=True, batch_size=None,\n",
    "                  **kwargs):\n",
    "        \"\"\"Translate a text sentences by sentences, preserving the fillers around the sentences.\"\"\"\n",
    "        if by_sentence:\n",
    "            sents, fillers = sentenize_with_fillers(text, splitter=self.splitter, ignore_errors=True)\n",
    "        else:\n",
    "            sents = [text]\n",
    "            fillers = [\"\", \"\"]\n",
    "        if preprocess:\n",
    "            sents = [self.preprocessor(sent) for sent in sents]\n",
    "        results = []\n",
    "        if not translate_single and batch_size: # Batch inference\n",
    "            current_translate_idxs = []\n",
    "            current_translate_sents = []\n",
    "            for idx, sent in enumerate(sents):\n",
    "                if len(sent.strip()):\n",
    "                    current_translate_idxs.append(idx)\n",
    "                    current_translate_sents.append(sent)\n",
    "                if (idx == len(sents) - 1 or len(current_translate_idxs) == batch_size) and len(current_translate_idxs):\n",
    "                    # tranaslate\n",
    "                    current_results = self.translate_vi2en(current_translate_sents, max_length_generation, **kwargs)\n",
    "                    assert len(current_results) == len(current_translate_idxs), \"Something wrong, not align len\"\n",
    "                    for idx_result, sent_result in zip(current_translate_idxs, current_results):\n",
    "                         sents[idx_result] = sent_result\n",
    "\n",
    "            for sent, sep in zip(sents, fillers):\n",
    "                results.append(sep)\n",
    "                results.append(sent.strip())\n",
    "\n",
    "            results.append(fillers[-1])\n",
    "            # torch.cuda.empty_cache()\n",
    "            return \"\".join(results)\n",
    "\n",
    "        else: # Single inference\n",
    "            for sent, sep in zip(sents, fillers):\n",
    "                results.append(sep)\n",
    "                if len(sent.strip()):\n",
    "                    results.append(self.translate_vi2en(sent, max_length_generation, **kwargs)[0])\n",
    "                else:\n",
    "                    results.append(sent)\n",
    "\n",
    "            results.append(fillers[-1])\n",
    "            # torch.cuda.empty_cache()\n",
    "            return \"\".join(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def translate_vi2en(self, text, max_length_generation, **kwargs):\n",
    "        input_ids = self.tokenizer(text,\n",
    "                                    padding=True,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    pad_to_multiple_of=8,\n",
    "                                    max_length=MAX_LENGTH)\n",
    "\n",
    "        maximum_token_current_batch = input_ids.input_ids.shape[1]\n",
    "        num_input_ids = input_ids.input_ids.shape[0]\n",
    "        if max_length_generation == \"auto\":\n",
    "            max_length_generation = int(10 + 1.2* maximum_token_current_batch)\n",
    "\n",
    "        out = []\n",
    "        if maximum_token_current_batch <= 128:\n",
    "          ideal_batch_size = 8\n",
    "        elif maximum_token_current_batch <= 192:\n",
    "          ideal_batch_size = 4\n",
    "        else:\n",
    "          ideal_batch_size = 1\n",
    "\n",
    "        for i in range(0, num_input_ids, ideal_batch_size):\n",
    "          # Create mini-batch\n",
    "          mini_input_ids = {}\n",
    "          for key, value in input_ids.items():\n",
    "              mini_input_ids[key] = value[i:min(i+ideal_batch_size, num_input_ids)]\n",
    "\n",
    "          mini_input_ids = BatchEncoding(data=mini_input_ids)\n",
    "\n",
    "          generated_tokens = self.model.generate(\n",
    "              **mini_input_ids.to(self.device),\n",
    "              decoder_start_token_id=self.tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "              max_length=max_length_generation,\n",
    "              num_return_sequences=1,\n",
    "              num_beams=5,\n",
    "              early_stopping=True,\n",
    "              **kwargs,\n",
    "          )\n",
    "          out.extend(self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_DATASET = len(dataset)\n",
    "NUM_PER_PART_DATASET = (LEN_DATASET + PART_DATASET - 1 ) //  PART_DATASET\n",
    "START_IDX = (INDEX_DATASET - 1) * NUM_PER_PART_DATASET\n",
    "END_IDX = min(START_IDX + NUM_PER_PART_DATASET, LEN_DATASET)\n",
    "\n",
    "print(\"******** RUNNING **********\")\n",
    "print(f\"Part {INDEX_DATASET} of {PART_DATASET}\")\n",
    "print(f\"From {START_IDX} to {END_IDX}\")\n",
    "print(\"***************************\")\n",
    "\n",
    "dataset = dataset.select(range(START_IDX, END_IDX))\n",
    "\n",
    "MODEL_URL = \"vinai/vinai-translate-vi2en-v2\"\n",
    "FILE_NAME = f\"law_translated-{INDEX_DATASET}-of-{PART_DATASET}.json\"\n",
    "OUTPUT_FOLDER = os.path.join(CURRENT_FOLDER_PATH, \"data\")\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "FILE_OUTPUT_PATH = os.path.join(OUTPUT_FOLDER, FILE_NAME)\n",
    "SAVE_PER_SAMPLES = 20\n",
    "\n",
    "# PUSH TO HF\n",
    "PUSH_TO_HF_PER_SAMPLES = 300\n",
    "HF_REPO_PATH = \"nntoan209/law_translated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "api.create_repo(repo_id=HF_REPO_PATH,\n",
    "                private=True,\n",
    "                repo_type=\"dataset\",\n",
    "                exist_ok=True)\n",
    "\n",
    "# Download\n",
    "try:\n",
    "    hf_hub_download(repo_id=HF_REPO_PATH,\n",
    "                    token=HF_TOKEN,\n",
    "                    repo_type=\"dataset\",\n",
    "                    local_dir=OUTPUT_FOLDER,\n",
    "                    filename=FILE_NAME,\n",
    "                    local_files_only=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Warning, not exist files on huggingface\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(MODEL_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue translating ..\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "IS_CONTINUE = True\n",
    "all_the_data = []\n",
    "\n",
    "if os.path.exists(FILE_OUTPUT_PATH) and IS_CONTINUE:\n",
    "    print(f\"\\nAlready exist file {os.path.basename(FILE_OUTPUT_PATH)}, continue translating ...\")\n",
    "    with open(FILE_OUTPUT_PATH) as fIn:\n",
    "        all_the_data = json.load(fIn)\n",
    "\n",
    "count = len(all_the_data)\n",
    "for data in tqdm(dataset.select(range(len(all_the_data), len(dataset)))):\n",
    "\n",
    "    text_en = translator.translate(data[\"text\"], translate_single=False, batch_size=INITIAL_BATCH_SIZE)\n",
    "\n",
    "\n",
    "    result_translated = {\"id\": data[\"id\"],\n",
    "                         \"text\": text_en\n",
    "                         }\n",
    "\n",
    "    all_the_data.append(result_translated)\n",
    "\n",
    "    count += 1\n",
    "    if not count % SAVE_PER_SAMPLES:\n",
    "        print(f\"\\nWriting output to {os.path.basename(FILE_OUTPUT_PATH)} with {len(all_the_data)} samples ... \")\n",
    "        with open(FILE_OUTPUT_PATH, \"w\") as fOut:\n",
    "            json.dump(all_the_data, fOut, ensure_ascii=False, indent=2)\n",
    "        print(\"Writing done\")\n",
    "\n",
    "    if not count % PUSH_TO_HF_PER_SAMPLES:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=FILE_OUTPUT_PATH,\n",
    "            path_in_repo=FILE_NAME,\n",
    "            repo_id=HF_REPO_PATH,\n",
    "            repo_type=\"dataset\")\n",
    "        print(f\"In Push to {os.path.basename(FILE_OUTPUT_PATH)} to HF ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nWriting output to {os.path.basename(FILE_OUTPUT_PATH)} with {len(all_the_data)} samples ... \")\n",
    "with open(FILE_OUTPUT_PATH, \"w\") as fOut:\n",
    "    json.dump(all_the_data, fOut, ensure_ascii=False, indent=4)\n",
    "print(\"Writing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toannn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
