{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from functools import partial \n",
    "import os\n",
    "NUM_PROC=max(os.cpu_count()-2,1)\n",
    "load_dataset=partial(datasets.load_dataset,num_proc=NUM_PROC,cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output/metadata_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalo_ds:datasets.Dataset=load_dataset(path=\"json\",data_files=\"./data/zalo/legal_corpus.json\")\n",
    "print(zalo_ds[\"train\"][0])\n",
    "nds1=[]\n",
    "originals=[]\n",
    "dataset_map=\"zalo_legal_corpus\"\n",
    "for law in tqdm(zalo_ds[\"train\"]):\n",
    "    for article in law[\"articles\"]:\n",
    "        article[\"law_id\"]=law[\"law_id\"]\n",
    "        article[\"oid\"]=len(nds1)\n",
    "        originals.append(article)\n",
    "        nds1.append({\n",
    "            \"text\":article[\"text\"],\n",
    "            \"dataset\":dataset_map,\n",
    "            \"oid\":article[\"oid\"]\n",
    "        })\n",
    "pq.write_table(pa.Table.from_pylist(originals),f\"output/metadata_map/{dataset_map}\")\n",
    "del originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_ds:datasets.Dataset=load_dataset(path=\"parquet\",data_files=\"./data/sft_v2.parquet\",split=\"train\")\n",
    "dataset_map=f\"sft_tvpl\"\n",
    "originals=[]\n",
    "sft_ds[0]\n",
    "for item in tqdm(sft_ds):\n",
    "    # item[\"text\"]=\" \".join(item[\"context\"])\n",
    "    item[\"contextoid\"]=[]\n",
    "    for text in item[\"context\"]:\n",
    "        item[\"contextoid\"].append(len(nds1))\n",
    "        nds1.append({\n",
    "            \"text\":text,\n",
    "            \"dataset\":dataset_map,\n",
    "            \"oid\":item[\"contextoid\"][-1]\n",
    "        })\n",
    "    originals.append(item)\n",
    "pq.write_table(pa.Table.from_pylist(originals),f\"output/metadata_map/{dataset_map}\")\n",
    "del originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvpl_ds = load_dataset(\"parquet\", data_files=\"data/TVPL/structured_data_doc.parquet\")\n",
    "print(tvpl_ds[\"train\"][0])\n",
    "dataset_map=\"tvpl_structured\"\n",
    "originals=[]\n",
    "for item in tqdm(tvpl_ds[\"train\"]):\n",
    "    children=item[\"data\"]\n",
    "    for id in range(len(children)):\n",
    "        children[id][\"text\"]=children[id][\"full_text\"]\n",
    "        del children[id][\"full_text\"]\n",
    "        children[id][\"oid\"]=len(nds1)\n",
    "        nds1.append({\n",
    "            \"text\":children[id][\"text\"],\n",
    "            \"dataset\":dataset_map,\n",
    "            \"oid\":children[id][\"oid\"]\n",
    "        })\n",
    "    item[\"data\"]=children \n",
    "    originals.append(item)\n",
    "pq.write_table(pa.Table.from_pylist(originals),f\"output/metadata_map/{dataset_map}\")\n",
    "del originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./temp/temp_legal_corpus_withstpvl.json\",\"w+\") as f:\n",
    "#     f.write(json.dumps(nds1))\n",
    "#     f.write(\"\\n\")\n",
    "#     f.flush() \n",
    "# union_ds=datasets.Dataset.from_list(nds1)\n",
    "!mkdir -p ./temp\n",
    "pq.write_table(pa.Table.from_pylist(nds1),\"./temp/temp_legal_corpus_withstpvl\")\n",
    "del nds1\n",
    "del zalo_ds\n",
    "del sft_ds\n",
    "del tvpl_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset(path=\"parquet\",data_files=\"./temp/temp_legal_corpus_withstpvl\")[\"train\"]\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JACCARD_THRESH=0.52\n",
    "MINHASH_THRESH=0.4\n",
    "JACCARD_MODE=\"PREVIOUS\"\n",
    "OUTPUT_PATH=\"output/merged_zalo_sftlaw_tvpl_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'python -m minhash \\\n",
    "  --path \"parquet\" \\\n",
    "  --data_files \"temp/temp_legal_corpus_withstpvl\"\\\n",
    "  --name \"gl\" \\\n",
    "  --split \"train\" \\\n",
    "  --cache_dir \"./cache\" \\\n",
    "  --output {OUTPUT_PATH} \\\n",
    "  --column \"text\" \\\n",
    "  --threshold {MINHASH_THRESH} \\\n",
    "  --min_length 5\\\n",
    "  --batch_size 10000 \\\n",
    "  --lsh_false_positive_thresh_weight 0.5 \\\n",
    "  --jaccard_mode {JACCARD_MODE} \\\n",
    "  --jaccard_thresh {JACCARD_THRESH}\\\n",
    "    --num_proc {NUM_PROC}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m minhash \\\n",
    "  --path \"parquet\" \\\n",
    "  --data_files \"temp/temp_legal_corpus_withstpvl\"\\\n",
    "  --name \"gl\" \\\n",
    "  --split \"train\" \\\n",
    "  --cache_dir \"./cache\" \\\n",
    "  --output {OUTPUT_PATH} \\\n",
    "  --column \"text\" \\\n",
    "  --threshold {MINHASH_THRESH} \\\n",
    "  --min_length 5\\\n",
    "  --batch_size 10000 \\\n",
    "  --lsh_false_positive_thresh_weight 0.5 \\\n",
    "  --jaccard_mode {JACCARD_MODE} \\\n",
    "  --jaccard_thresh {JACCARD_THRESH}\\\n",
    "  --num_proc {NUM_PROC}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clid=set()\n",
    "merged_ds=load_dataset(\"parquet\",data_files=f\"{OUTPUT_PATH}_reindexed({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")[\"train\"]\n",
    "for item in tqdm(merged_ds):\n",
    "    clid.add(item[\"__cluster__\"])\n",
    "print(len(clid))\n",
    "del clid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds=load_dataset(\"parquet\",data_files=f\"{OUTPUT_PATH}_reindexed({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")[\"train\"]\n",
    "seencluster=set()\n",
    "for _ in range(5):\n",
    "    duplicated_id=-1\n",
    "    for item in merged_ds:\n",
    "        if(item[\"__cluster__\"]!=item[\"oid\"] and item[\"__cluster__\"] not in seencluster):\n",
    "            duplicated_id=item[\"__cluster__\"]\n",
    "            break\n",
    "    for item in merged_ds.filter(lambda item: item[\"__cluster__\"]==duplicated_id,num_proc=NUM_PROC):\n",
    "        print(item)\n",
    "    seencluster.add(duplicated_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds=load_dataset(\"parquet\",data_files=f\"{OUTPUT_PATH}_reindexed({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")[\"train\"]\n",
    "seencluster=set()\n",
    "for _ in range(5):\n",
    "    duplicated_id=-1\n",
    "    for item in merged_ds:\n",
    "        if(item[\"__cluster__\"]!=item[\"oid\"] and item[\"dataset\"]==\"sft_tvpl\" and item[\"__cluster__\"] not in seencluster):\n",
    "            duplicated_id=item[\"__cluster__\"]\n",
    "            break\n",
    "    for item in merged_ds.filter(lambda item: item[\"__cluster__\"]==duplicated_id,num_proc=NUM_PROC):\n",
    "        print(item)\n",
    "    seencluster.add(duplicated_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterlen={}\n",
    "for item in tqdm(merged_ds):\n",
    "    cluster=item[\"__cluster__\"]\n",
    "    clusterlen[cluster]=clusterlen.get(cluster,0)+1\n",
    "lendist={}\n",
    "for k,v in tqdm(clusterlen.items()):\n",
    "    lendist[v]=lendist.get(v,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lendist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds=load_dataset(\"parquet\",data_files=f\"{OUTPUT_PATH}_reindexed({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")[\"train\"]\n",
    "try:\n",
    "    merged_ds=merged_ds.remove_columns([\"__score__\",\"oparent\"])\n",
    "except:\n",
    "    pass\n",
    "merged_ds=merged_ds.sort(\"__cluster__\")\n",
    "precount=0\n",
    "toKeep={}\n",
    "# \"zalo_legal_corpus\"\n",
    "# \"tvpl_structured\"\n",
    "clusterMember={}\n",
    "def isPreferedDataset(ds_label):\n",
    "    return ds_label[0:3]==\"sft\" or ds_label==\"zalo_legal_corpus\"\n",
    "for item in tqdm(merged_ds,desc=\"Getting oid to keep\"):\n",
    "    precount+=1\n",
    "    pre=toKeep.get(item[\"__cluster__\"],(-1,-1,\"\"))\n",
    "    clusterMember[item[\"__cluster__\"]]=clusterMember.get(item[\"__cluster__\"],[])+[item[\"oid\"]]\n",
    "    cur=(len(item[\"text\"]),item[\"oid\"],item[\"dataset\"])\n",
    "    if pre[0]<0:\n",
    "        toKeep[item[\"__cluster__\"]]=cur\n",
    "    elif isPreferedDataset(cur[2]):\n",
    "        if (not isPreferedDataset(pre[2])) or pre[0]<cur[0]:\n",
    "            toKeep[item[\"__cluster__\"]]=cur\n",
    "    else:\n",
    "        if (not isPreferedDataset(pre[2])) and pre[0]<cur[0]:\n",
    "            toKeep[item[\"__cluster__\"]]=cur\n",
    "print(toKeep)\n",
    "keep_oid=[tokeepitem[1] for tokeepitem in toKeep.values()]\n",
    "keep_oid=set(keep_oid)\n",
    "print(precount)\n",
    "print(len(keep_oid))\n",
    "import gc\n",
    "gc.freeze()\n",
    "gc.disable()\n",
    "filtered=merged_ds.filter(\n",
    "    function=lambda item: item[\"oid\"] in keep_oid,\n",
    "    num_proc=4,\n",
    "    desc=\"Filtering clusters...\",\n",
    ").map(\n",
    "    function= lambda item: {\"__cluster_member__\": clusterMember[item[\"__cluster__\"]]},\n",
    "    num_proc=4,\n",
    "    desc=\"Adding cluster member IDs ...\"\n",
    ")  \n",
    "gc.enable()\n",
    "gc.collect()\n",
    "pq.write_table(pa.Table.from_pylist(filtered),f\"{OUTPUT_PATH}_filtered({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAP=f\"({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\"\n",
    "!mkdir -p \"output/data_remapped_{REMAP}\"\n",
    "# DONOT USE the FILTERED DATASET HERE \n",
    "merged_ds=load_dataset(\"parquet\",data_files=f\"{OUTPUT_PATH}_reindexed({MINHASH_THRESH}|{JACCARD_THRESH}|{JACCARD_MODE})\")[\"train\"]\n",
    "oid_cluster_dict={}\n",
    "for item in tqdm(merged_ds):\n",
    "    oid_cluster_dict[item[\"oid\"]]=item[\"__cluster__\"]\n",
    "\n",
    "old_ds=load_dataset(\"parquet\",data_files=\"output/metadata_map/tvpl_structured\")[\"train\"]\n",
    "def tvpl_mapper(item,ocdict):\n",
    "    children=item[\"data\"]\n",
    "    for id in range(len(children)):\n",
    "        children[id][\"__cluster__\"]=ocdict.get(children[id][\"oid\"],children[id][\"oid\"])\n",
    "    item[\"data\"]=children\n",
    "    return item\n",
    "old_ds=old_ds.map(\n",
    "    function=tvpl_mapper,\n",
    "    fn_kwargs={\n",
    "        \"ocdict\":oid_cluster_dict\n",
    "    },\n",
    "    num_proc=NUM_PROC,\n",
    "    desc=\"Writing file tvpl_structured\"\n",
    ")\n",
    "old_ds.to_parquet(f\"output/data_remapped_{REMAP}/tvpl_dataset\")\n",
    "\n",
    "old_ds=load_dataset(path=\"parquet\",data_files=\"output/metadata_map/sft_tvpl\")[\"train\"]\n",
    "def sft_tpvl_mapper(item,ocdict):\n",
    "    item[\"__context_cluster__\"]=[]\n",
    "    for oid in item[\"contextoid\"]:\n",
    "        item[\"__context_cluster__\"].append(ocdict.get(oid,oid))\n",
    "    return item\n",
    "old_ds=old_ds.map(\n",
    "    function=sft_tpvl_mapper,\n",
    "    fn_kwargs={\n",
    "        \"ocdict\":oid_cluster_dict\n",
    "    },\n",
    "    num_proc=NUM_PROC,\n",
    "    desc=\"Writing file tvpl_structured\"\n",
    ")\n",
    "old_ds.to_parquet(f\"output/data_remapped_{REMAP}/sft_tvpl\")\n",
    "\n",
    "\n",
    "def data_mapper(item,ocdict):\n",
    "    item[\"__cluster__\"]=ocdict.get(item[\"oid\"],item[\"oid\"])\n",
    "    return item\n",
    "for file_name in [\"zalo_legal_corpus\"]:\n",
    "    old_ds=datasets.load_dataset(\"parquet\",data_files=f\"output/metadata_map/{file_name}\")[\"train\"]\n",
    "    old_ds=old_ds.map(\n",
    "        function=data_mapper,\n",
    "        fn_kwargs={\n",
    "            \"ocdict\":oid_cluster_dict\n",
    "        },\n",
    "        num_proc=NUM_PROC,\n",
    "        desc=f\"Writing file {file_name}\"     \n",
    "    )\n",
    "    old_ds.to_parquet(f\"output/data_remapped_{REMAP}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IN=f\"output/data_remapped_{REMAP}/sft_tvpl\"\n",
    "TARGET_OUT=f\"output/data_remapped_{REMAP}/sft_tvpl_out\"\n",
    "! python -m minhash \\\n",
    "  --path \"parquet\" \\\n",
    "  --data_files \"{TARGET_IN}\"\\\n",
    "  --name \"tvpl_sft\" \\\n",
    "  --split \"train\" \\\n",
    "  --cache_dir \"./cache\" \\\n",
    "  --output \"{TARGET_OUT}\" \\\n",
    "  --column \"question\" \\\n",
    "  --threshold 0.4 \\\n",
    "  --min_length 5\\\n",
    "  --batch_size 10000 \\\n",
    "  --lsh_false_positive_thresh_weight 0.5 \\\n",
    "  --jaccard_mode \"PREVIOUS\" \\\n",
    "  --jaccard_thresh 0.52\\\n",
    "  --num_proc {NUM_PROC}\\\n",
    "  --cluster_column \"__cluster_question__\"\\\n",
    "  --index_column \"questionoid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft=load_dataset(\"parquet\",data_files=TARGET_OUT+\"_reindexed(0.4|0.52|PREVIOUS)\")[\"train\"]\n",
    "seencluster=set()\n",
    "for _ in range(5):\n",
    "    duplicated_id=-1\n",
    "    for item in sft:\n",
    "        if(item[\"__cluster_question__\"]!=item[\"questionoid\"] and  item[\"__cluster_question__\"] not in seencluster):\n",
    "            duplicated_id=item[\"__cluster_question__\"]\n",
    "            break\n",
    "    for item in sft.filter(lambda item: item[\"__cluster_question__\"]==duplicated_id,num_proc=NUM_PROC):\n",
    "        print(item)\n",
    "    seencluster.add(duplicated_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft=load_dataset(path=\"parquet\",data_files=TARGET_OUT+\"_reindexed(0.4|0.52|PREVIOUS)\")[\"train\"]\n",
    "# CLUSTER_COL=\"__cluster_question__\"\n",
    "# sft=sft.map(\n",
    "#     function=lambda x: {\"_len_\":len(x[\"text\"])},\n",
    "#     num_proc=NUM_PROC\n",
    "# )\n",
    "# sft=sft.sort([CLUSTER_COL,\"_len_\"])\n",
    "# pre=sft[0][CLUSTER_COL]\n",
    "# keeplist=[]\n",
    "# for i,item in tqdm(enumerate(sft)):\n",
    "#     if item[CLUSTER_COL]==pre:\n",
    "#         continue\n",
    "#     else:\n",
    "#         keeplist.append(sft[i-1])\n",
    "# new_sft=datasets.Dataset.from_list(keeplist)\n",
    "# new_sft=new_sft.map(\n",
    "#     remove_columns=[\"_len_\",\"__cluster_question__\"]\n",
    "# )\n",
    "# ns=new_sft.train_test_split(test_size=10000)\n",
    "# OUTPUT_DIR_SFT=f\"output/data_remapped_{REMAP}/tpvl_sft_dedup_and_resplit\"\n",
    "# os.makedirs(exist_ok=True,name=OUTPUT_DIR_SFT)\n",
    "# ns[\"train\"].to_parquet(os.path.join(OUTPUT_DIR_SFT,\"train.parquet\"))\n",
    "# ns[\"test\"].to_parquet(os.path.join(OUTPUT_DIR_SFT,\"test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft=load_dataset(path=\"parquet\",data_files=TARGET_OUT+\"_reindexed(0.4|0.52|PREVIOUS)\")[\"train\"]\n",
    "CLUSTER_COL=\"__cluster_question__\"\n",
    "TEST_SIZE=10000\n",
    "sortedsft=sft.sort(\"__cluster_question__\")\n",
    "split_index=len(sortedsft)-TEST_SIZE\n",
    "precluster=sortedsft[split_index][CLUSTER_COL]\n",
    "while split_index>0 and precluster==sortedsft[split_index-1][CLUSTER_COL]:\n",
    "    split_index-=1\n",
    "if split_index<=0:\n",
    "    raise Exception()\n",
    "OUTPUT_DIR_SFT=f\"output/data_remapped_{REMAP}/tpvl_sft_dedup_and_resplit\"\n",
    "os.makedirs(exist_ok=True,name=OUTPUT_DIR_SFT)\n",
    "def traingen():\n",
    "    i=0\n",
    "    while(i<split_index):\n",
    "        yield sortedsft[i]\n",
    "        i+=1\n",
    "def testgen():\n",
    "    i=split_index\n",
    "    while(i<len(sortedsft)):\n",
    "        yield sortedsft[i]\n",
    "        i+=1\n",
    "datasets.Dataset.from_generator(traingen,cache_dir=\"cache\",num_proc=NUM_PROC).to_parquet(os.path.join(OUTPUT_DIR_SFT,\"train.parquet\"))\n",
    "datasets.Dataset.from_generator(testgen,cache_dir=\"cache\",num_proc=NUM_PROC).to_parquet(os.path.join(OUTPUT_DIR_SFT,\"test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds:datasets.Dataset=datasets.load_dataset(path=\"parquet\",data_files=\"output/data_remapped/tvpl_dataset\")[\"train\"]\n",
    "# print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpvl_dataset=datasets.load_dataset(\"json\",data_files=\"output/data_remapped/tvpl_structured\")[\"train\"]\n",
    "# precount=0\n",
    "# parentnochild_count=0\n",
    "# toKeep={}\n",
    "# for item in tpvl_dataset:\n",
    "#     for child in item[\"child_data\"]:\n",
    "#         pre=toKeep.get(child[\"__cluster__\"],(-1,-1))\n",
    "#         if pre[0]<len(child[\"text\"]):\n",
    "#             toKeep[child[\"__cluster__\"]]=(len(child[\"text\"]),child[\"oid\"])\n",
    "# tpvl_filtered=[]\n",
    "# for item in tpvl_dataset:\n",
    "#     children=item[\"child_data\"]\n",
    "#     new_children=[]\n",
    "#     for id in range(len(children)):\n",
    "#         cid=children[id][\"__cluster__\"]\n",
    "#         precount+=1\n",
    "#         if children[id][\"oid\"]==toKeep[cid][1]:\n",
    "#             new_children.append(children[id])\n",
    "#     if len(new_children)==0:\n",
    "#         parentnochild_count+=1\n",
    "#         del item[\"child_data\"]\n",
    "#         print(\"NO CHILD:\",json.dumps(item,ensure_ascii=False))\n",
    "#     else:\n",
    "#         item[\"child_data\"]=new_children\n",
    "#         tpvl_filtered.append(item)\n",
    "\n",
    "# pq.write_table(pa.Table.from_pylist(tpvl_filtered),\"output/data_remapped/tvpl_dataset_filtered\")\n",
    "# del tpvl_filtered\n",
    "# print(precount)\n",
    "# print(len(toKeep))\n",
    "# print(parentnochild_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "300965d21fb88682e4cf3ff55bff3ad6798c8a8898f69517064ae3adad6c8abf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
